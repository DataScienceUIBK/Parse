# Parse: An Open-Domain Reasoning Question Answering Benchmark for Persian

[![Hugging Face Dataset](https://img.shields.io/badge/HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/JamshidJDMY/Parse)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Paper](https://img.shields.io/badge/Paper-PDF-blue)](interface/An_Open_Domain_Reasoning_Question_Answering_Benchmark_for_Persian.pdf)

Parse is an **open-domain Persian reasoning QA benchmark** designed to evaluate reasoning-capable QA systems and LLMs in a low-resource language setting.  
It includes diverse question formats (Boolean / Multiple-choice / Factoid), reasoning dimensions (Multihop / Reasoning), and difficulty levels (Easy / Medium / Hard).

This repository contains:
- The dataset files (train/test/full)
- Prompt templates used for question generation
- Evaluation scripts for **Zero-shot**, **Few-shot**, and **Chain-of-Thought** prompting
- Fine-tuning utilities (TogetherAI formatting & uploader script)
- Human evaluation interfaces for quality and difficulty validation

---

## ðŸ“Œ Dataset on HuggingFace

You can download the dataset directly from HuggingFace:

**ðŸ¤— HuggingFace Dataset:** `JamshidJDMY/Parse`  
https://huggingface.co/datasets/JamshidJDMY/Parse

---

## ðŸš€ Quick Start (HuggingFace Datasets)

Install dependencies:

```bash
pip install datasets
```

Load dataset:

```python
from datasets import load_dataset

ds = load_dataset("JamshidJDMY/Parse")

print(ds)
print(ds["train"][0])
```

---

## ðŸ” Reproducibility (Minimal Setup)

This repo includes ready-to-run scripts for evaluation under:
- **Zero-shot**
- **Few-shot**
- **Chain-of-Thought**

### 1) Install environment

It is recommended to use Python 3.10+:

```bash
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
# .venv\Scripts\activate    # Windows
```

Install core requirements:

```bash
pip install -U pip
pip install datasets numpy tqdm pandas scikit-learn
```

> If you use API-based models (Together / OpenAI / etc.), you may need to install additional SDKs
> and set the corresponding API keys depending on your setup.

---

### 2) Run evaluation scripts

#### âœ… Zero-shot
```bash
cd evaluation/zero_shot
bash boolean_sh.sh
bash multichoice_sh.sh
bash factoid_sh.sh
```

#### âœ… Few-shot
```bash
cd evaluation/few_shot
bash boolean_sh.sh
bash multichoice_sh.sh
bash factoid_sh.sh
```

#### âœ… Chain-of-Thought (CoT)
```bash
cd evaluation/chain_of_thought
bash boolean_sh.sh
bash multichoice_sh.sh
bash factoid_sh.sh
```

---

### 3) Evaluate the generated predictions

Each evaluation setup contains:
- `evaluate_results.py`
- `evaluate_finetuned_results.py`

Example:

```bash
python evaluate_results.py
```

---

### 4) Output format

Outputs are saved as JSON files in:

```bash
evaluation/<setting>/prompt_results/<task>/<language>/
```

Example:

```bash
evaluation/chain_of_thought/prompt_results/boolean/persian/answers_llama-3-70b.json
```

---

## ðŸ“‚ Repository Structure

```bash
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ full.json
â”‚   â”œâ”€â”€ train.json
â”‚   â””â”€â”€ test.json
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ Boolean_*.txt
â”‚   â”œâ”€â”€ Factoid_*.txt
â”‚   â””â”€â”€ Multichoice_*.txt
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ zero_shot/
â”‚   â”œâ”€â”€ few_shot/
â”‚   â”œâ”€â”€ chain_of_thought/
â”‚   â”œâ”€â”€ human_quality_evaluation/
â”‚   â””â”€â”€ human_difficulty_validation/
â”‚
â”œâ”€â”€ finetune/
â”‚   â”œâ”€â”€ to_together_ai.py
â”‚   â”œâ”€â”€ english_prompt/
â”‚   â”œâ”€â”€ persian_prompt/
â”‚   â””â”€â”€ together_ai_data_format/
â”‚       â””â”€â”€ train_together.jsonl
â”‚
â””â”€â”€ interface/
    â”œâ”€â”€ difficulty_evalation_interface.html
    â”œâ”€â”€ quality_evaluation_interface.html
    â””â”€â”€ QA_Annotation_Guide.pdf
```

---

## ðŸ“œ Citation

If you use Parse, please cite our paper:

```bibtex
@inproceedings{mozafari2026parse,
  title={Parse: An Open-Domain Reasoning Question Answering Benchmark for Persian},
  author={Mozafari, Jamshid and Mousavinasab, Seyed Parsa and Jatowt, Adam},
  booktitle={Proceedings of the 49th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},
  year={2026}
}
```

---

## ðŸ“„ License
This project is released under the license provided in the repository. See [LICENSE](LICENSE).
